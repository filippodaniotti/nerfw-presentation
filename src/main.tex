\documentclass[aspectratio=1610]{beamer}
\usepackage{theme}
\usepackage[utf8x]{inputenc}

\usepackage[english]{babel}
\usepackage{calligra}
\usepackage{lmodern}
\usepackage{subcaption}
\usepackage{hyperref}

\usepackage{amsmath,amssymb,amsfonts,physics}
\usepackage{array, booktabs, makecell}
\usepackage{tabularx}

\usepackage{textcomp}
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

% Graphics and video
\usepackage{graphicx,float,wrapfig}
\graphicspath{
  {./assets/images/}%
}


% args: big, bigg, Big, Bigg
\newcommand{\parenth}[2][]{#1(#2#1)}
\renewcommand{\bold}[1]{\textbf{\structure{#1}}}
  
\author[Daniotti \and Lucerno]{Filippo Daniotti \and Taylor Lucerno}
\title[NeRF-W]{\textsc{NeRF in the Wild}}
\subtitle{Neural Radiance Fields for Unconstrained Photo Collections}
\institute[DISI - UniTN]{Department of Information Engineering\\and Computer Science}
\date{\today}


\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}

\begin{document}

\begin{frame}
    \titlepage
    \begin{figure}[H]
        \begin{center}
            \includegraphics[width=0.4\linewidth]{marchio_unitrento_colore_it_202002.eps}
        \end{center}
    \end{figure}
\end{frame}


\begin{frame}
    \tableofcontents[sectionstyle=show,subsectionstyle=show/shaded/hide,subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Introduction}
\begin{frame}{Introduction}
    Content:
    \begin{itemize}
        \item problem statement
        \item NeRF
        \item NeRF in the Wild
    \end{itemize}
\end{frame}

\section{The Problem}
\begin{frame}
    \begin{itemize}
        \item synthesizing novel views of a scene from a sparse set of captured images
        \item doing it in unconstrained environment:
        \begin{itemize}
            \item scene varies amongst training set (illumination, shading, tone)
            \item scene is partially occluded by obstacles
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Background: Neural Radiance Fields}
\begin{frame}{Core concepts}
    \begin{block}{In a nutshell}
        NeRFs represent a continuous complex scene as a function 
        \begin{equation*}
            F_\theta : (\vb{x}, \vb{d}) \rightarrow (\vb{c}, \sigma)
        \end{equation*}
    \end{block}
    This function is approximated with a MLP \(F_\theta \rightarrow\)  overfitted to encode only scene!
    % \pause
    \bigskip
    \begin{figure}[H]
        \centering
        \includegraphics[width=.7\textwidth,keepaspectratio]{mapping}
    \end{figure}
\end{frame}

\begin{frame}{Core concepts: synthesizing novel views}
    \begin{columns}
        \begin{column}{.5\textwidth}
            \begin{figure}[H]
                \includegraphics[width=.7\textwidth,keepaspectratio]{density-1}
            \end{figure}
        \end{column}
        \pause
        \begin{column}{.5\textwidth}
            \begin{figure}[H]
                \includegraphics[width=.7\textwidth,keepaspectratio]{density-2}
            \end{figure}
        \end{column}
    \end{columns}
    \begin{onlyenv}<3->
        \begin{block}{Volume rendering with Radiance Fields} 
            Approximate the expected color \(\hat{\vb{C}}(\vb{r})\) encountered by camera ray \(\vb{r}(t) = \vb{o} + t\vb{d}\)
            \begin{equation*}
                \hat{\vb{C}}(\vb{r}) = \mathcal{R}(\vb{r}, \vb{c}, \sigma) = \sum_{k = 1}^{K} T(t_k) \alpha (\sigma (t_k)\delta_k) \vb{c}(t_k)
            \end{equation*}
        \end{block}
    \end{onlyenv}
\end{frame}

\begin{frame}{Architecture}
    Quick note on the architecture:
    \begin{itemize}
        \item the original NeRF uses one single MLP
        \item NeRF-W uses two different MLPs the better enforce the dependencies
        \begin{itemize}
            \item one to estimate density \(\sigma(t)\) and a vector \(\vb{z}(t)\) from the location
            \item one to estimate the color \(\vb{c}(t)\) from the viewing direction and the vector \(\vb{z}(t)\)
        \end{itemize}
    \end{itemize}
    \begin{block}{MLPs}
        \begin{align}
            [\sigma(t), \vb{z}(t)] &= \textrm{MLP}_{\theta_1} (\gamma_{\vb{x}}(\vb{r}(t))) \\
            \vb{c}(t) &= \textrm{MLP}_{\theta_2} (\vb{z}(t), \gamma_{\vb{d}}(\vb{d}))
        \end{align}
    \end{block}
\end{frame}

\begin{frame}{Optimizing NeRFs}
    A few tricks are employed:
    \begin{itemize}
        \item Positional encoding: location vector \(\vb{x}\) is mapped to a higher dimensional space in order to more easily approximate higher-frequency functions
        \item Hierarchical volume sampling: two MLP are actually employed, a coarse one (first) and then a fine one (sampling points for its rays are sampled after the coarse has run for a given training epoch) 
    \end{itemize}
    \begin{block}{Training loss}
        \begin{equation}
            \sum_{\vb{r} \in \mathcal{R}} ||\vb{C}(\vb{r}) - \vb{\hat{C}}^c(\vb{r})||_2^2 + ||\vb{C}(\vb{r}) - \vb{\hat{C}}^f(\vb{r})||_2^2
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}{NeRF Limitations}
    NeRFs assume that the real world scenes are geometrically, phonometrically and materially static, which is often not the case
    \begin{itemize}
        \item phonometric variations: two photographs of a scene taken at different times of day, different periods of the year, atmospheric conditions will vary in terms of illumination, shading, ton quality
        \item transient objects: many real world landmakrs will have a crowd around, possibly captured in the shots
    \end{itemize}
    NeRFs in the wild address these two issues
\end{frame}

\section{Paper contribution: NeRF in the Wild}
\begin{frame}{}

    

\end{frame}

\section{Limitations, possible improvements, future work}
\begin{frame}
    \begin{center}
        Thanks for your attention
    \end{center}
\end{frame}

\end{document}