\documentclass[aspectratio=1610,handout]{beamer}
\usepackage{theme}
\usepackage[utf8x]{inputenc}

\usepackage[english]{babel}
\usepackage{calligra}
\usepackage{lmodern}
\usepackage{subcaption}
\usepackage{hyperref}
% \usepackage[inline]{enumitem}

\usepackage{amsmath,amssymb,amsfonts,physics}
\usepackage{array, booktabs, makecell}
\usepackage{tabularx}

\usepackage{textcomp}
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

% Graphics and video
\usepackage{graphicx,float,wrapfig}
\graphicspath{
  {./assets/images/}%
}


% args: big, bigg, Big, Bigg
\newcommand{\parenth}[2][]{#1(#2#1)}
\renewcommand{\bold}[1]{\textbf{\structure{#1}}}
  
\author[Daniotti \and Lucero]{Filippo Daniotti \and Taylor Lucero}
\title[NeRF-W]{\textsc{NeRF in the Wild}}
\subtitle{Neural Radiance Fields for Unconstrained Photo Collections}
\institute[DISI - UniTN]{Department of Information Engineering\\and Computer Science}
\date{\today}


\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}

\begin{document}

\begin{frame}
    \titlepage
    \begin{figure}[H]
        \begin{center}
            \includegraphics[width=0.4\linewidth]{marchio_unitrento_colore_it_202002.eps}
        \end{center}
    \end{figure}
\end{frame}


\begin{frame}
    \tableofcontents[sectionstyle=show,subsectionstyle=show/shaded/hide,subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Introduction}
\begin{frame}{Introduction}
    Content:
    \begin{itemize}
        \item problem statement
        \item NeRF
        \item NeRF in the Wild
    \end{itemize}
\end{frame}

\section{The Problem}
\begin{frame}
    \begin{itemize}
        \item synthesizing novel views of a scene from a sparse set of captured images
        \item doing it in unconstrained environment:
        \begin{itemize}
            \item scene varies amongst training set (illumination, shading, tone)
            \item scene is partially occluded by obstacles
        \end{itemize}
    \end{itemize}
\end{frame}

\section{Background: Neural Radiance Fields}

\subsection*{Core concepts}
\begin{frame}{Core concepts: representing a scene as a NeRF}
    \begin{block}{In a nutshell}
        NeRFs represent a continuous complex scene as a function 
        \begin{equation*}
            F_\theta : (\vb{x}, \vb{d}) \rightarrow (\vb{c}, \sigma)
        \end{equation*}
    \end{block}
    This function is approximated with a MLP \(F_\theta \rightarrow\)  overfitted to encode only scene!
    % \pause
    \bigskip
    \begin{figure}[H]
        \centering
        \includegraphics[width=.7\textwidth,keepaspectratio]{mapping}
    \end{figure}
\end{frame}

\begin{frame}{Core concepts: synthesizing novel views}
    \begin{columns}
        \begin{column}{.5\textwidth}
            \begin{figure}[H]
                \includegraphics[width=.7\textwidth,keepaspectratio]{density-1}
            \end{figure}
        \end{column}
        \pause
        \begin{column}{.5\textwidth}
            \begin{figure}[H]
                \includegraphics[width=.7\textwidth,keepaspectratio]{density-2}
            \end{figure}
        \end{column}
    \end{columns}
    \begin{onlyenv}<3->
        \begin{block}{Volume rendering with Radiance Fields} 
            Approximate the expected color \(\hat{\vb{C}}(\vb{r})\) encountered by camera ray \(\vb{r}(t) = \vb{o} + t\vb{d}\)
            \begin{equation*}
                \hat{\vb{C}}(\vb{r}) = \mathcal{R}(\vb{r}, \vb{c}, \sigma) = \sum_{k = 1}^{K} T(t_k) \alpha (\sigma (t_k)\delta_k) \vb{c}(t_k)
            \end{equation*}
        \end{block}
    \end{onlyenv}
\end{frame}

\subsection*{Model}
\begin{frame}{Architecture}
    \begin{onlyenv}<1>
        \begin{figure}[H]
            \centering
            \includegraphics[width=.8\textwidth]{nerf-architecture}
        \end{figure}
    \end{onlyenv}

    \pause
    
    \begin{onlyenv}<2->
        \begin{columns}
            \begin{column}{.7\textwidth}
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=.65\textwidth]{nerf-architecture}
                \end{figure}
            \end{column}
            \begin{column}{.3\textwidth}
                \Large{Why?}
            \end{column}
        \end{columns}
        \vspace{1.2cm}

        \pause

        \uncover<3->{
            \begin{columns}
                \begin{column}{.3\textwidth}
                    \centering
                    \includegraphics[width=.75\textwidth]{architecture-why-1.png}
                \end{column}
                \pause
                \begin{column}{.3\textwidth}
                    \centering
                    \includegraphics[width=.75\textwidth]{architecture-why-2.png}
                \end{column}
                \pause
                \begin{column}{.3\textwidth}
                    \centering
                    \includegraphics[width=.9\textwidth]{architecture-why-3.png}
                \end{column}
            \end{columns}
        }
    \end{onlyenv}
    % Quick note on the architecture:
    % \begin{itemize}
    %     \item the original NeRF uses one single MLP
    %     \item NeRF-W uses two different MLPs the better enforce the dependencies
    %     \begin{itemize}
    %         \item one to estimate density \(\sigma(t)\) and a vector \(\vb{z}(t)\) from the location
    %         \item one to estimate the color \(\vb{c}(t)\) from the viewing direction and the vector \(\vb{z}(t)\)
    %     \end{itemize}
    % \end{itemize}
    % \begin{block}{MLPs}
    %     \begin{align}
    %         [\sigma(t), \vb{z}(t)] &= \textrm{MLP}_{\theta_1} (\gamma_{\vb{x}}(\vb{r}(t))) \\
    %         \vb{c}(t) &= \textrm{MLP}_{\theta_2} (\vb{z}(t), \gamma_{\vb{d}}(\vb{d}))
    %     \end{align}
    % \end{block}
\end{frame}

\begin{frame}{Optimizing NeRFs}
    A few tricks are employed
    \bigskip
    \pause
    \begin{columns}[t]
        \begin{column}{.42\textwidth}
            \textbf{Positional encoding}\\
            map \(\vb{x}\), \(\vb{d}\) to a higher-dimensional space
            \begin{align*}
                [\sigma(t), \vb{z}(t)] & = \textrm{MLP}_{\theta_1} (\alert<2>{\gamma}_{\vb{x}}(\vb{x})) \\
                \vb{c}(t) & = \textrm{MLP}_{\theta_2} (\vb{z}(t), \alert<2>{\gamma}_{\vb{d}}(\vb{d}))
            \end{align*}
        \end{column}
        \pause
        \begin{column}{.42\textwidth}
            \textbf{Hierarchical volume sampling}\\
            use 2 MLPs sequentially
            \begin{align*}
                &\vb{\hat{C}}^c(\vb{r}) \rightarrow \textrm{coarse one}\\
                &\vb{\hat{C}}^f(\vb{r}) \rightarrow \textrm{fine one}
            \end{align*}
        \end{column}
    \end{columns}
    % \begin{itemize}
    %     \item Positional encoding: location vector \(\vb{x}\) is mapped to a higher dimensional space in order to more easily approximate higher-frequency functions
    %     \item Hierarchical volume sampling: two MLP are actually employed, a coarse one (first) and then a fine one (sampling points for its rays are sampled after the coarse has run for a given training epoch) 
    % \end{itemize}
    \bigskip
    \pause
    \begin{block}{Training objective}
        \begin{equation}
            \sum_{\vb{r} \in \mathcal{R}} ||\vb{C}(\vb{r}) - \vb{\hat{C}}^c(\vb{r})||_2^2 + ||\vb{C}(\vb{r}) - \vb{\hat{C}}^f(\vb{r})||_2^2
        \end{equation}
    \end{block}
\end{frame}

\begin{frame}{NeRF Limitations}
    NeRFs assume real world scenes to be \textbf{static}
    \vspace{1.5cm}
    \begin{columns}[t]
        \begin{column}{.4\textwidth}
            Photometric variations\\
        \end{column}
        \begin{column}{.4\textwidth}
            Transient objects\\
        \end{column}
    \end{columns}
    \begin{columns}
        \begin{column}{.4\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{issues-var.png}
            \end{center}
        \end{column}
        \begin{column}{.4\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{issues-transient.png}
            \end{center}
        \end{column}
    \end{columns}
    % \begin{itemize}
    %     \item phonometric variations: two photographs of a scene taken at different times of day, different periods of the year, atmospheric conditions will vary in terms of illumination, shading, ton quality
    %     \item transient objects: many real world landmakrs will have a crowd around, possibly captured in the shots
    % \end{itemize}
    \vspace{1.5cm}
    \pause
    NeRF in the wild addresses these two issues
\end{frame}

\section{NeRF in the Wild}
\begin{frame}{Contributions}
    NeRF-W builds up on NeRF and adds the following ideas:
    \bigskip
    \pause
    \begin{itemize}
        \item make radiance \(\vb{c}_i (t)\) image-dependent \(\rightarrow\) allow variations in \(\vb{\hat{C}}_i\) between images 
        \pause 
        \item add a \emph{transient} head \(\rightarrow\) emits its own color \emph{and density}
        \begin{itemize}
            \item transient density varies between images
            \item weighted by uncertainty field
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection*{Latent appearence modeling}
\begin{frame}{Latent appearence modeling}
    \begin{block}{Generative Latent Optimization}
        Assign a \(\ell_i^{(a)}\) 
        %of length \(n^{(a)}\) 
        appearence embedding vector to each image \(\mathcal{I}_i\)
        \begin{equation*}
            \vb{c}_i(t) = \textrm{MLP}_{\theta_2} (\vb{z}(t), \gamma_{\vb{d}}(\vb{d}), \alert{\ell_i^{(a)}})
        \end{equation*}
        Optimize \(\ell_i^{(a)}\) alongside \(\theta\)
    \end{block}
    \begin{onlyenv}<2>
        \bigskip
        \begin{figure}[H]
            \centering
            \includegraphics[width=.65\textwidth,keepaspectratio]{nerfa-architecture.png}
        \end{figure}
    \end{onlyenv}
\end{frame}

\begin{frame}{The appearence embedding vector}
    \begin{itemize}
        \item \(\ell_i^{(a)}\) fed only to MLP\(_{\theta_2}\) 
        \begin{itemize}
            \item keep 3D geometry prediction static
            \item modulate emitted radiance and ensure smooth interpolation
        \end{itemize} 
    \end{itemize}
    \bigskip
    \begin{figure}[H]
        \centering
        \includegraphics[width=.2\textwidth,keepaspectratio]{nerfa-results-1.png}
        \hspace{4cm}
        \includegraphics[width=.2\textwidth,keepaspectratio]{nerfa-results-3.png}\\
        \includegraphics[width=.9\textwidth,keepaspectratio]{nerfa-results-2.png}
    \end{figure}
\end{frame}

\subsection*{Transient component}
\begin{frame}{Transient Head}
    Transient phenomena alter image density
    \bigskip
    \begin{center}
        \includegraphics[width=.5\textwidth]{issues-transient.png}
    \end{center}
    \bigskip
    \pause
    \begin{columns}
        \begin{column}{.5\textwidth}
            \begin{itemize}
                \item we need a way to disentangle static vs. transient representation of the scene
                \begin{itemize}
                    \item add a separate MLP head
                \end{itemize}
            \end{itemize}
            \begin{itemize}
                \item transient representation should be image-dependent
                \begin{itemize}
                    \item use a transient embedding vector \(\ell_i^{(\tau)}\)
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{center}
                \includegraphics[width=.9\textwidth]{mlp3.png}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Uncertainty field}
    \begin{block}{NeRF-W volume rendering}
        \begin{equation*}
            \hat{\vb{C}}(\vb{r}) = \sum_{k = 1}^{K} T(t_k) ( \alpha (\sigma (t_k)\delta_k)  \vb{c}(t_k)  \alert<1>{+ (\sigma_i^{(\tau)} (t_k)\delta_k)  \vb{c}_i^{(\tau)}(t_k)})
        \end{equation*}
    \end{block}\
    \bigskip
    But... 
    \pause
    \begin{itemize}
        \item not all pixels are equally reliable
        \begin{itemize}
            \item learn a \emph{uncertainty field} \(\beta_i\)
            \item adapt reconstruction loss to ignore unreliable pixels and 3D locations likely to contain occluders
            %TODO don't forget this!
            \item actual color is modeled as a normal distribution with \(\beta_i\) as variance
            % rendered on transiente component as usual\(\hat{\beta}_i (\vb{r}) = \mathcal{R}(\vb{r}, \beta_i, \sigma_i^{(\tau)})\)
        \end{itemize}
    \end{itemize}
    \bigskip
    \pause
    \begin{figure}[H]
        \centering
        \includegraphics[width=.75\textwidth]{render.png}
    \end{figure}
\end{frame}

\begin{frame}{NeRF-W full architecture}
    \begin{equation*}
        \bigg[\sigma_i^{(\tau)} (t), \vb{c}_i^{(\tau)} (t), \tilde{\beta}_i (t)\bigg] = \textrm{MLP}_{\theta_3} \parenth[\bigg]{\vb{z}(t), \ell_i^{(\tau)}},
        \quad\quad 
        \beta_i (t) = \beta_{\textrm{min}} + \textrm{softplus}\parenth[\bigg]{\tilde{\beta}_i (t)}
    \end{equation*}
    \begin{figure}[H]
        \centering
        \includegraphics[width=.65\textwidth,keepaspectratio]{nerfw-architecture.png}
    \end{figure}
\end{frame}

\subsection{Optimizing NeRF-W}
\begin{frame}{Optimization}
    \begin{block}{Objective}
        \begin{itemize}
            \item Fine model: loss for ray \(\vb{r}\) in image \(i\) with true color \(\vb{C}(\vb{r})\)
            \begin{equation*}
                L_i(\vb{r}) 
                = \frac{|| \vb{C}_i (\vb{r}) - \hat{\vb{C}}_i (\vb{r}) ||_2^2}{2\beta_i (\vb{r})^2} 
                + \frac{\log \beta_i (\vb{r})^2}{2} 
                + \frac{\lambda_u}{K} \sum^K_{k=1}\sigma_i^{(\tau)} (t_k)
            \end{equation*}
            \item Coarse model: only uses latent appearance modeling component
            \begin{equation*}
                \sum_{ij} L_i (\vb{r}_{ij}) + \frac{1}{2} || \vb{C} (\vb{r}_{ij}) - \hat{\vb{C}}_i^c (\vb{r}_{ij}) ||^2_2 
            \end{equation*}
            \item hyperparameters: \(n_i^{(a)}\), \(n_i^{(\tau)}\), \(\beta_{\textrm{min}}\), \(\lambda_u\)
        \end{itemize}
    \end{block}  
\end{frame}

\subsection{Results}
\begin{frame}{Phototourism dataset}
    \begin{center}
        \includegraphics[width=.7\textwidth]{res-photo.png}
    \end{center}
\end{frame}
\begin{frame}{Lego dataset}
    \begin{center}
        \includegraphics[width=.51\textwidth]{results-lego.png}
    \end{center}
\end{frame}

\section{Our contribution}
\begin{frame}[plain,noframenumbering]
    \begin{center}
        Thanks for your attention
    \end{center}
\end{frame}


\end{document}